{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./Chinese-LLaMA-2-7B-hf\", use_fast=False, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3a3aa",
   "metadata": {},
   "source": [
    "# Part2: The LLM generate Stage\n",
    "\n",
    "- author: lyuchuny3@foxmail.com\n",
    "- 知乎-link:https://zhuanlan.zhihu.com/p/1921601639858562902"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fe630",
   "metadata": {},
   "source": [
    "## 1. LLaMA模型架构概览\n",
    "LLaMA模型分为开头的embedding和后续的N个block (这个7B模型有32个blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be45778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./Chinese-LLaMA-2-7B-hf\", \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3454c7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(40076, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=40076, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aceb482",
   "metadata": {},
   "source": [
    "\n",
    "## 2. generate生成流程概览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecf39894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    1, 29871, 30672, 31823, 33205, 33646, 31753]]),\n",
       " torch.Size([1, 7]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=\"我喜欢攀岩\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "x = inputs.input_ids #shape [1,7]\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad381ec3",
   "metadata": {},
   "source": [
    "以上面的输入为例，输入是input_ids的shape为[1,7]，则生成的流程是\n",
    "\n",
    "\n",
    "1. [`embed_tokens`]： 将 token ID 映射为 embedding\n",
    "2. [`layers 0-31`]: 逐层计算 hidden states。\n",
    "3. [`norm`]: 最终归一化 hidden states\n",
    "4. [`lm_head`]: 将 hidden states 映射为 logits\n",
    "5. [`取最后一个位置的 logits`]:只关注序列最后一个位置的 logits\n",
    "6. [`采样 token ID`]:根据 logits 采样下一个 token ID\n",
    "7. [`循环`]: 将新 token ID 添加到序列中，重复步骤 1-6.直到满足停止条件，生成完整序列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befebbc6",
   "metadata": {},
   "source": [
    "### 1. Embedding 层（embed_tokens）​​\n",
    "将输入的 token ID（整数）映射为对应的向量表示（embedding）。\n",
    "```python3\n",
    "输入 input_ids 的 shape 是 [1, 7]：（batch size = 1， seq_len =7)\n",
    "输出 hidden_states 的 shape 是 [1, 7, 4096]：(embedding_dim =4096)\n",
    "```\n",
    "计算过程，embeding的权重是 weight [40076,4096]，则经过Gather [weight, inp]得到 output[1，7, 4096]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460e9de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "199de84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  torch.Size([1, 7])\n",
      "hid_state.shape:  torch.Size([1, 7, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(\"x.shape: \",x.shape)\n",
    "hid_stat = model.model.embed_tokens(x) \n",
    "print(\"hid_state.shape: \", hid_stat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5d276",
   "metadata": {},
   "source": [
    "### 2. Transformer Decoder 层（layers 0-31）​​\n",
    "对 embed_tokens 输出的 hidden states 进行多层处理，逐步提取更高层次的语义信息。\n",
    "```\n",
    "输入和输出为hidden_states，shape 都为 [1, 7, 4096]\n",
    "```\n",
    "每一层 LlamaDecoderLayer 包含以下组件：Self-Attention，MLP, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38d3ac5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaDecoderLayer(\n",
       "  (self_attn): LlamaAttention(\n",
       "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (mlp): LlamaMLP(\n",
       "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "    (act_fn): SiLUActivation()\n",
       "  )\n",
       "  (input_layernorm): LlamaRMSNorm()\n",
       "  (post_attention_layernorm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7835970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 4096])\n"
     ]
    }
   ],
   "source": [
    "hid_stat = model.model.layers[0](hid_stat)[0]      # [1, 7, 4096]\n",
    "print(hid_stat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016410f7",
   "metadata": {},
   "source": [
    "### 3. Final LayerNorm（norm）​​\n",
    "对最后一层 LlamaDecoderLayer 输出的 hidden states 进行最终的归一化，确保数值稳定性。\n",
    "```\n",
    "输入和输出为hidden_states，shape 都为 [1, 7, 4096]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64d2dcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 4096])\n"
     ]
    }
   ],
   "source": [
    "hid_stat = model.model.norm(hid_stat)              # [1, 7, 4096]\n",
    "print(hid_stat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e733a36",
   "metadata": {},
   "source": [
    "### 4. Language Model Head（lm_head）​​\n",
    "lm_head 是一个线性层，其作用是将 hidden states 映射为 logits（词表上的得分），用于预测下一个 token 。\n",
    "```\n",
    "输入：hidden_states，shape 为 [1, 7, 4096]（来自 norm）。\n",
    "输出：logits，shape 为 [1, 7, 40076]：\n",
    "```\n",
    "注意：logits 表示模型对当前输入序列中每个位置上所有可能的 token 的得分（未归一化的概率分布）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bcb98ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 40076])\n"
     ]
    }
   ],
   "source": [
    "hid_stat = model.lm_head(hid_stat)                 # [1, 7, 40076]\n",
    "print(hid_stat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db21341a",
   "metadata": {},
   "source": [
    "### 5. 取最后一个位置的 logits\n",
    "​在自回归语言模型中，我们通常只关心序列中​​最后一个位置​​的 logits，因为模型是​​因果（causal）​​的：\n",
    "\n",
    "模型在生成第 t 个 token 时，只能看到序列中第 0 到第 t-1 个 token 的信息。\n",
    "\n",
    "因此，在采用之前，从logits [1, 7, 40076] 取最后一个位置的 logits，也就是logits [1, 40076]，这个作为采用的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30670b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hid_stat.shape: torch.Size([1, 7, 40076])\n",
      "last logit shape: torch.Size([1, 40076])\n"
     ]
    }
   ],
   "source": [
    "logics = hid_stat[:,-1,:]\n",
    "print(\"hid_stat.shape:\",hid_stat.shape)\n",
    "print(\"last logit shape:\",logics.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f80a07",
   "metadata": {},
   "source": [
    "### 6. Sampling（采样）​\n",
    "采样是为了从 logits 中采样下一个 token ID，作为模型预测的结果。\n",
    "```\n",
    "输入：logits [1, 40076] \n",
    "输出：1  # next token_id\n",
    "```\n",
    "根据 model.generate() 中的参数：\n",
    "\n",
    "- do_sample=True,   启用采样策略（而不是直接取最大值）\n",
    "- top_k=10,         只从 logits 中得分最高的 10 个 token 中进行采样\n",
    "- top_p=0.85,       使用核采样（nucleus sampling），只从累计概率达到 85% 的 token 中进行采样\n",
    "- temperature=1     控制采样的“随机性”，temperature=1 表示不调整概率分布。scores = scores / self.temperature\n",
    "\n",
    "采样过程包括：\n",
    "\n",
    "- 对 logits 应用 temperature（缩放 logits）。\n",
    "- 可选：应用 top_k 和 top_p 过滤，限制采样的 token 范围。\n",
    "- 通过 softmax 将 logits 转换为概率分布。\n",
    "- 使用 torch.multinomial 从概率分布中采样一个 token ID。\n",
    "\n",
    "采样得到的 token ID 是模型预测的下一个 token。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32cd5a",
   "metadata": {},
   "source": [
    "### 7. 自回归生成（Autoregressive Generation）​​\n",
    "循环执行上述1-6步骤（从 embed_tokens 到采样），逐步生成完整的 token 序列。\n",
    "\n",
    "- 初始输入 input_ids=[1, 7]\n",
    "- 执行前向计算，采样得到第 8 个 token ID。\n",
    "- 将新生成的 token ID 添加到输入序列中，形成新的输入（如 [1, 7]->[1,8] ）。\n",
    "- 重复上述过程，直到满足停止条件（如达到 max_new_tokens 或遇到 eos_token_id）。\n",
    "\n",
    "最终生成的 token ID 序列就是 generate_ids。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f17a97",
   "metadata": {},
   "source": [
    "## 3. Prefill和Decoder过程\n",
    "```\n",
    "问题：prefill和decoder对应到这个生成过程的哪些执行过程呢？\n",
    "```\n",
    "Prefill 和 Decoder对应的是demo代码里的model.generate(）这一行过程。\n",
    "\n",
    "对大模型推理进行**性能优化**的时候，会在生成阶段区分Prefill 和 Decoder：\n",
    "\n",
    "​- ​Prefill（预填充）阶段​​\n",
    "​- ​Decoder（解码 / 自回归生成）阶段​\n",
    "\n",
    "对应到第二章节的生成过程，如果我的输入是7个tokens，输出是10个tokens，那么生成过程是"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9facdc0",
   "metadata": {},
   "source": [
    "```python\n",
    "prompt=\"我喜欢攀岩\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "x = inputs.input_ids  #shape [1,7]\n",
    "\n",
    "# prefill #######################################################\n",
    "hid_stat = model.model.embed_tokens(x)             # [1, 7, 4096]\n",
    "hid_stat = model.model.layers[0](hid_stat)[0]      # [1, 7, 4096]\n",
    "...\n",
    "hid_stat = model.model.layers[31](hid_stat)[0]     # [1, 7, 4096]\n",
    "hid_stat = model.model.norm(hid_stat)              # [1, 7, 4096]\n",
    "hid_stat = model.lm_head(hid_stat)                 # [1, 7, 40076]\n",
    "\n",
    "logics = hid_stat[:,-1，：]    # get last token logis #[1, 40076]\n",
    "next_token_id = sample(logics) #[1]\n",
    "\n",
    "#decoder #########################################################\n",
    "x_new # shape[1,1]\n",
    "hid_stat_new = model.model.embed_tokens(x_new)      # [1, 1, 4096]\n",
    "# ... layer 0-31, norm                              # [1, 1, 4096]\n",
    "hid_stat_new = model.lm_head(hid_stat_new)          # [1, 1, 40076]\n",
    "logits = hid_stat_new[:,-1，：]                       # [1, 1, 40076]\n",
    "next_token_id = sample(logics) #[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241e990",
   "metadata": {},
   "source": [
    "1. ​​Prefill（预填充）阶段：​​\n",
    "- 当传入 inputs.input_ids（即 prompt 对应的 token ID 序列）时，模型需要一次性计算出这个输入序列中​​所有 token 的 hidden states（隐藏状态）​​。\n",
    "- 这个过程称为 ​​Prefill​​，因为它“预先填充”了输入部分的计算。\n",
    "- 在这个阶段，模型会计算整个输入序列的 attention 和 hidden states，并缓存这些中间结果（通常称为 KV Cache），以便后续解码阶段复用。\n",
    "\n",
    "2. ​​Decoder（解码 / 自回归生成）阶段：​​\n",
    "- 在 Prefill 完成之后，模型开始​​逐个生成新的 token​​，也就是所谓的“自回归生成”。\n",
    "- 每次生成一个新的 token，模型都会基于之前所有已生成的 token（包括 prompt 的 token 和已经生成的部分）来预测下一个 token。\n",
    "- 这个过程是​​一步一步（token-by-token）​​进行的，直到达到 max_new_tokens 或遇到 eos_token_id（句子结束符）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d572442",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
